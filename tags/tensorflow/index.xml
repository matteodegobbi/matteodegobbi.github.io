<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tensorflow on Matteo De Gobbi</title><link>https://matteodegobbi.github.io/tags/tensorflow/</link><description>Recent content in Tensorflow on Matteo De Gobbi</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 16 Oct 2023 02:06:54 +0200</lastBuildDate><atom:link href="https://matteodegobbi.github.io/tags/tensorflow/index.xml" rel="self" type="application/rss+xml"/><item><title>Dataset Inference attacks for Generative Adversarial Networks ðŸ¤–</title><link>https://matteodegobbi.github.io/p/dataset-inference-attacks-for-generative-adversarial-networks/</link><pubDate>Mon, 16 Oct 2023 02:06:54 +0200</pubDate><guid>https://matteodegobbi.github.io/p/dataset-inference-attacks-for-generative-adversarial-networks/</guid><description>&lt;img src="https://matteodegobbi.github.io/gan.png" alt="Featured image of post Dataset Inference attacks for Generative Adversarial Networks ðŸ¤–" />&lt;p>In my bachelor&amp;rsquo;s thesis I explored the idea of membership inference attacks, where we try to determine whether a given sample was present in the training set of a neural network without having access to the weights of the neural network itself. In particular I targeted Generative Adversarial Networks where the training data can contain sensitive information, especially in the medical setting, in forensics and criminal justice.&lt;/p>
&lt;p>The full thesis can be found at: &lt;a class="link" href="https://thesis.unipd.it/handle/20.500.12608/57083" target="_blank" rel="noopener"
>https://thesis.unipd.it/handle/20.500.12608/57083&lt;/a>&lt;/p>
&lt;h2 id="abstract">Abstract:
&lt;/h2>&lt;p>Generative Adversarial Networks (GANs) have had great success in the generation of artifical samples from datasets made of sensitive data which can&amp;rsquo;t be disclosed publicly. These GANs, if released to the public, could allow an attacker to leak sensitive information from the GAN&amp;rsquo;s training dataset. We analize a type of attack called Membership Inference Attack (MIA), which consists of determining the membership of a certain sample to the training set of the GAN. We analize the success of both black box and white box Membership Inference Attacks on GANs trained on MNIST and anime faces. We look for a relationship between the precision of the attacks and the several hyperparameters of the GANs such as: amount of images in the training set, number of epochs of training, quality of generated images, number of generated images available to the attacker. We show how an insufficient number of training images or an excessive number of training epochs causes overfitting in the GAN, which is then vulnerable to MIAs. We analize how the FrÃ©chet&amp;rsquo;s Inception Distance (FID) between the set of generated images and the original training set impacts on the success of the MIAs.&lt;/p></description></item></channel></rss>