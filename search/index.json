[{"content":"In February of this year we published a paper on undescribed Insect species and genus classification using DNA and image data.\nMy main contributions, together with Roger, were:\nReACGAN for image feature extraction CNN model with vertical kernels for DNA feature extraction Compiling of finetuning dataset for the ReACGAN and CNN, scraping data from BOLDSystemsV3. (Image+DNA) Compiling of pretraining dataset for the ReACGAN, merging previous datasets. (Only Image) Replicating a previous study on the new dataset, since the dataset of the original paper is not publicly available The finetuning dataset can be found at: https://zenodo.org/records/14277812\nThe pretraining dataset can be found at: https://zenodo.org/records/14577906\nThe code can be found at: https://github.com/matteodegobbi/InsectClassification\nPaper: https://www.mdpi.com/1999-4893/18/2/105\\\n","date":"2025-06-18T00:51:54+02:00","image":"https://matteodegobbi.github.io/p/paper-on-insect-species-and-genus-classification/img_hu_d58dc02bc02d1d69.png","permalink":"https://matteodegobbi.github.io/p/paper-on-insect-species-and-genus-classification/","title":"Paper on Insect Species and Genus Classification üêùüêûüêõ"},{"content":"Producer-Consumer problem with dynamic rate adjustment In this repo I implemented the producer consumer problem with dynamic rate adjustment carried out with an actor.\nThe consumer is a thread that consumes messages at a given fixed rate, that is, with a given delay from a shared buffer, simulating the consumed message usage. The producer is a thread that creates messages and writes them to a shared buffer, the rate of the producer is controlled by another thread called the actor.\nAn actor separate from producer and consumer periodically checks the message queue length and if the length is below a given threshold, it will increase the production rate. Otherwise (i.e. the message length is above the given threshold), it will decrease the production rate. In this case the message queue is implemented with a circular queue (or ring buffer).\nThe main idea is to try to keep the buffer always half-full. Meaning the objective is to have buffer utilization always close to 50%, this leaves room in both ways: if the consumer is delayed we will still have room for inserting new data, and if the producer is delayed we still have data to consume for a while. This way it maximizes the production rate until the buffer utilization goes over 50%. Any other threshold could be chosen and the program still works (e.g. I tried also with 70%).\nI tried two approaches, both inspired by control systems, the control is carried out by changing a delay in the producer thread, this will dynamically adjust the rate of the producer to try to keep the buffer utilization at 50%.\nThe two controllers are:\nA simple threshold based control system A PID controller Both systems share the code for the producer and the consumer. The actor that carries out the control is different in the two solutions. In both cases the actor runs at a fixed rate (e.g. every 1ms or 0.5ms etc.) and it checks the current buffer utilization, it then uses this information to adjust the producer\u0026rsquo;s rate. More information about the two methods will be given below.\nIt\u0026rsquo;s important to have the actor\u0026rsquo;s rate not too high or too low. If the delay is too high then the actor does not have time to react to changes in buffer utilization, leading to oscillations between the buffer being too full and too empty. Inversely if the delay is too small the actor will enter the critical section too often, making the producer and consumer wait to enter their critical section and degrading the perfomance of the two.\nUnfortunately we cannot fix a rate for the actor for every task and every system and this will need to be tuned depending on the system used.\nAnother important detail is the length of the buffer: if the buffer is too small and the producer (or consumer) is carrying out some operation, the other thread will empty (or fill) the buffer and we don\u0026rsquo;t have enough room to provide some leeway for the two threads to do their work while the other one is waiting or performing some other operations, hampering the concurrency of the system. In most of my experiments I tried buffer sizes of 100000 and 1000000. This problem can be witnessed with a buffer size of just 1000 or 100 the buffer utilization oscillates between 0% and 100%. The buffer length also influences at what rate we can run the actor, a bigger buffer allows us to run the actor less often, meaning the actor adds less overhead to the system.\nControl methods In this section I explain the two methods I implemented to try to maximize the production rate while keeping the buffer utilization under 50%.\nNaive threshold based method The first method I implemented is called naive_actor in the code and it loops over the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 pthread_mutex_lock(\u0026amp;mutex); int count = (writeIdx - readIdx + buffer_size) % buffer_size; float percent = (float)count / (float)buffer_size; if (percent \u0026lt; 0.45) { producer_delay *= 0.8; producer_delay += 1; // to avoid having delay==0 } else if (percent \u0026gt; 0.55) { producer_delay *= 1.2; } producer_delay = producer_delay \u0026gt; 1e6 ? 1e6 : producer_delay; // clip if above 1s int last_delay = (int)producer_delay; pthread_mutex_unlock(\u0026amp;mutex); this is a very simple implementation that just check whether the percent utilization of the buffer has gone above 55% or below 45% and decreases or increases the producer\u0026rsquo;s rate by 20%. This approach is similar to bang-bang control in control theory since it only activates when it\u0026rsquo;s between two thresholds, in this case the output is like a binary on/off that either increases or decreases the delay by 20% independently of how far the buffer utilization is from 50%.\nThe producer delay is clipped to be 1 second at most, this is not necessary with the normal setup with fixed consumer delay but later I will explain a variation I tried where the consumer\u0026rsquo;s delay is generated by a Cauchy random variable and in this case this naive controller can be unstable and create high delays above 1 second in the producer.\nWhen using the PID controller that I will explain later, this step is not needed, because even when the consumer\u0026rsquo;s delay is generated by a Cauchy r.v. the system is still stable without clipping.\nIn the figure below we can see how the naive controller performs, the buffer utilization reaches 50%, then it overshoots to ~57% and then starts oscillating between ~43% and ~57%, this is expected for two reasons:\nThe controller only works if the utilization goes outside the range [0.45,0.55], When it changes the delay it only reacts by changing it by a fixed percentage of the previous value. It does not use the distance from the target utilization or the rate of change of the error, meaning if the current utilization is 56% it will react in the same way if the previous utilization was 55% or if it was 90%. But it should react differently, if the previous utilization was 56% and it\u0026rsquo;s still 56% we probably need to increase the delay, but if it was 90% it means the consumer has increased its speed for whatever reason so we probably won\u0026rsquo;t need to increase the delay. This 2nd point made me think if I can incorporate in some way the rate of change, and at this point I realized that I can use a PID controller, this way I can use the rate of change through the derivative and I can also tune the parameters more easily, since I just need to change 3 numbers. Also using a PID controller the proportional term will be proportional to the error reacting differently if the error is small or large, the naive controller always reacts at the same speed independently from the magnitude of the error.\nFigure 1: Buffer utilization percentage and producer's delay when using the naive controller PID controller To implement the PID controller I changed the actor code and made the function pid_actor which changes the main loop to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 float prev_error = 0.0; float integral = 0.0; while (keep_running) { pthread_mutex_lock(\u0026amp;mutex); int count = (writeIdx - readIdx + buffer_size) % buffer_size; float percent = (float)count / (float)buffer_size; float error = 0.50 - percent; integral += error; float derivative = (error - prev_error); float output = k_p * error + k_d * derivative + k_i * integral; prev_error = error; producer_delay = producer_delay - (int)(output); producer_delay = producer_delay \u0026lt; 0 ? 0 : producer_delay; // clip if below 0 int last_delay = producer_delay; pthread_mutex_unlock(\u0026amp;mutex); // write buffer utilization and producer delay rate fprintf(fp, \u0026#34;%.2f,%d\\n\u0026#34;, percent, last_delay); usleep(ACTOR_DELAY); } The actor computes the error as the difference between the 0.50 (the wanted buffer utilization) and the current buffer utilization. Then this error is used to compute the output of the controller that will be subtracted from the current delay in a feedback loop. We have that the output is made up of three terms:\nProportional term which is proportional to the error itself, this is similar to the naive controller Integrative term, which means proportional to the sum of past errors Derivative term, meaning proportional to the difference between the current and previous error. (This is the part that incorporates the rate of change) The multiplicative constants K_p,K_i,K_d are read from a configuration file.\nIn this case we don\u0026rsquo;t need to clip the delay to be below one second as the system is already stable, but we do need to clip it to not go below 0 as a negative delay makes no sense.\nThis PID actor works much better without oscillations and it just requires to try a few combinations of the parameters to work. Some values of K_p,K_i,K_d I found that work well are 1000,0,1000, 1000,0,10000 or 10000,0,10000.\nThe constant K_d of the derivative term has the effect of dampening oscillations and overshoots, but if it\u0026rsquo;s increased too much it can make the system too slow to react to changes of regimes and to initially reach the 50% utilization.\nI found that the integral term is usually not needed as there isn\u0026rsquo;t a drift in time from the wanted output. The only set of parameters I found an instance in which the integral term helps: with K_p=100,K_d=10000 the system has a small steady state error so adding a small integral term of K_i=0.01 removes this steady state error at the cost of some oscillation. Since this steady state error can also be removed by increasing K_p without introducing oscillations I never wound up using the integral term but I left it implemented because in some systems it could be useful to counteract systemic noise.\nThis example of adding the effect of adding K_i=0.01 can be seen in the figure below.\nFigure 2a: Buffer utilization percentage with PID controller and Kp=100,Ki=0,Kd=10000. We can notice that the steady state of utilization is slightly above 50%, around 51% and no oscillation occurs. Figure 2b: Buffer utilization percentage with PID controller and Kp=100,Ki=0.01,Kd=10000. We can notice that the utilization oscillates around 50%. Figure 2c: Buffer utilization percentage with PID controller and Kp=1000,Ki=0,Kd=10000, the system quickly reaches 50% utilization and does not oscillate. Below I will show some plots when using the PID controller with different constants.\nFirsty let\u0026rsquo;s see with just a proportional controller:\nFigure 3: Buffer utilization percentage and producer's delay when using the PID controller with Kp=100,Ki=00,Kd=0, we have oscillations in both the utilization and the producer delay Since with just a proportional controller like in the previous figure there are oscillations we can try to add a derivative term to try to dampen the oscillations:\nFigure 4: Buffer utilization percentage and producer's delay when using the PID controller with Kp=100,Ki=0,Kd=1000, the oscillations decrease in magnitude over time. Eventually the producer's delay matches the consumer's. We can now try to increase Kp and/or Kd to try to see if we can get better outcomes in terms of reaching steady state faster and reducing overshoot and oscillations:\nFigure 5: Buffer utilization percentage and producer's delay when using the PID controller with Kp=1000,Ki=0,Kd=1000, the overshoot is decreased and the systems reaches steady state quickly, still there are oscillations in the producer's delay. To try to reduce the oscillations in the producer\u0026rsquo;s delay I finally increased both Kp and Kd to make the system settle fast but also reduce the oscillations in the controlling signal.\nFigure 6: Buffer utilization percentage and producer's delay when using the PID controller with Kp=10000,Ki=0,Kd=10000, the utilization doesn't overshoot and reaches steady state quickly, the oscillations in the producer's delay happen only at the beginning and quickly go away. Variable rate consumer using Cauchy distributed random delay Finally I decided to see if the two systems can deal with a consumer that has a very variable rate, to do this is decided to generate the delay of the consumer with a Cauchy random variable with center 100 and scale parameter 100.\nI choose a Cauchy rv because it has very high variability and is unpredictable, the median of this delay will still be 100 microseconds but much higher delays can be generated, still I clipped the delay to not be more than 1e6 microseconds (1 second) to avoid very long wait times.\nJust to see how variable this delay is we can run this R script:\n1 2 3 4 5 6 x_gaussian = rnorm(mean=100,sd=100,n=1000) x_cauchy = rcauchy(100,100,1000) print(median(x_gaussian)) print(median(x_cauchy)) print(max(x_gaussian)) print(max(x_cauchy)) with a run of this script we get that the medians are 107.7077 for the gaussian and 106.806 for the Cauchy. But the maxes are very different: 475.0348 for the gaussian and 55947.0 for the Cauchy. And this is with only 1000 samples, increasing the number of samples would highlight the difference even further as Cauchy r.v.\u0026rsquo;s don\u0026rsquo;t have a finite variance.\nSo the system needs to be able to act quickly in order to counteract this kind of spikes in delays.\nNow I will show some plots overviewing the behaviour of the two controllers when dealing with this Cauchy delay.\nFigure 7: Buffer utilization percentage and producer's delay when using the naive controller, the consumer's delay is generated by a Cauchy r.v. Figure 8: Buffer utilization percentage and producer's delay when using the PID controller with Kp=10000,Ki=0,Kd=10000, the consumer's delay is generated by a Cauchy r.v. As we can see from the previous Figures 7 and 8 the two controllers exhibit very different behaviours. The naive controller goes from delay 0 to 10 seconds of delay (this is the reason why after I added the clipping to 1 second) in some systems (e.g. my laptop) this is very drastic and can hang the system for a long time, the buffer utilization is always between 40% and 60% but it\u0026rsquo;s more unstable.\nWith the same setup the PID controller is much more stable having spikes only when the consumer has a long delay and quickly reaching 50% again by adjusting the producer rate. The controller also doesn\u0026rsquo;t reach very long delays as it can respond in a more stable manner, reaching at most 60000 microseconds delay against 1e7 microseconds (or more) of the naive controller.\nHow to run the experiments You need to have installed gcc and make in a POSIX compatible machine with pthreads (I tested the code on Ubuntu 24.04 LTS on a x86 machine and MacOS Sequoia 15.5 on a ARM machine)\nFor the plotting you need to have installed python with the libraries pandas and matplotlib.\nTo run the experiment you can set the PID constants in the text config file pid_constants.txt and run\n1 2 3 4 # to run with fixed delay make run-and-plot MODE=1 # to run with Cauchy delay make cauchy-run-and-plot MODE=1 The MODE argument when set to 1 (default value) runs with the PID controller and when set to 0 runs with the naive controller.\nTo set the durations of the experiment or the buffer size you can change the Makefile.\n","date":"2025-08-02T18:19:21+02:00","image":"https://matteodegobbi.github.io/example_plots/fixed_delay/pid_1000_1000/prod_delay.png","permalink":"https://matteodegobbi.github.io/p/dynamic-producer-consumer/","title":"Dynamic Producer Consumer üìàüìâ"},{"content":"This June I took part in the Digital Skills Cup AI Pro 2025, an online trivia competition about AI and Deep Learning hosted by LLPA. I got first place in the national qualification and then I also got first place in the world competition!\nI won a prize of 3000$ to use to travel abroad fully paid by LLPA.\nI visited Crete with my girlfriend, we had a lot of fun and it was a very entertaining experience participating in this competition.\nLinkedin post about the competition: https://www.linkedin.com/feed/update/urn:li:activity:7355870715205533698/\n","date":"2025-06-18T02:06:47+02:00","image":"https://matteodegobbi.github.io/p/first-place-winner-of-ai-pro-competition/cnosso_hu_2fa536b14fba324e.jpg","permalink":"https://matteodegobbi.github.io/p/first-place-winner-of-ai-pro-competition/","title":"First place winner of AI Pro Competition üèÜ"},{"content":"This May I partecipatd at HackUPC 2025, an hackhathon organized by FIB in Barcelona.\nTogether with Vojtƒõch, we built a reinforcement learning agent that learns to play a game similar to Geometry Dash\u0026rsquo;s rocket section. The agent is trained using deep Q-Learning and we built the game using pygame.\nWe managed to get in the top 5 and present our work in front of everyone üòÅ!\nDevpost: https://devpost.com/software/rocket-deep-reinforcement-learning\nCode: https://github.com/matteodegobbi/CaveDescentRL\nVojtƒõch\u0026rsquo;s webiste: https://vojtechbestak.cz/\\\nMe and Vojtƒõch explaining our project ","date":"2025-06-15T01:27:57+02:00","image":"https://matteodegobbi.github.io/p/reinforcement-learning-agent-for-cave-descent-game-at-hackupc-2025/img_hu_bac54a934ca30e92.png","permalink":"https://matteodegobbi.github.io/p/reinforcement-learning-agent-for-cave-descent-game-at-hackupc-2025/","title":"Reinforcement learning agent for Cave Descent game at HackUPC 2025"},{"content":"During the Summer school BOOST 24 I attended in Bologna (Read the BOOST post here ) professor Alessandro Panconesi told us about a fall school he would be attending later this year called BUCA 24, in which engineers from Google would be giving lectures about various interesting topics. I decided to apply and I got accepted!\nDuring this fall school we stayed in the beautiful island of San Servolo in Venice and we attended lectures about various techniqes that are used at Google to make their products better:\nSpeculation, caching and LoRA in Transformers based deep learning models Load Balancing with consistent hashing System design to ensure reliability and efficiency of systems with billions of users Paxos algorithms for consensus in distributed systems The lectures were held by:\nDan Ardelean, Area Tech Lead for Search Infrastructure, VP of Engineering at Google JJ Furman, engineer at Google and the founder and creator of Megastore, the storage system used by Gmail and Drive Amer Diwan, Area Tech Lead for Performance and Capacity for Search Infrastructure at Google Website of BUCA: https://buca24.bici.events/instructors\n","date":"2024-10-15T02:06:54+02:00","image":"https://matteodegobbi.github.io/sanservolo.jpeg","permalink":"https://matteodegobbi.github.io/p/fall-school-buca-2024/","title":"Fall School BUCA 2024 üçÇ"},{"content":"In the summer of 2024 I got invited by one of my university professors to partecipate in the summer school BOOST24 held in Bologna, in this eleven days we attended many lectures by professors coming from universities all over the world.\nMy favourite lectures I attended at BOOST were:\nHilbert, G√∂del, Turing: Computers \u0026lsquo;R\u0026rsquo; Us by professor Alessandro Panconesi from Sapienza Universit√† di Roma. In this series of lectures professor Panconesi went over the historical context behind the Hilbert Program, the project birthed in the 1900s by David Hilbert that tried to formalize the whole of mathematics starting from a finite complete set of fundamental axioms. Then the lecturer explained to us the negative answers that proved Hilbert\u0026rsquo;s program impossible: Turing\u0026rsquo;s answer to the Entscheidungsproblem and Godel\u0026rsquo;s incompleteness theorem. These lectures were incredibly interesting and showed us how much mathematics has changed in the last 100 years. These lectures also sparked many interesting conversation at dinner time with the other students attending.\nMachine Learning for the Rest of Us by professor Keshav Pingali from University of Texas at Austin. Professor Pingali explained to us reinforcement learning starting from the very basics of ML, Deep Learning and finally arriving to reinforcement learning. Explaining Markov Decision Processes and how to solve the full RL problem, taking the point of view of optimal control.\nWebpage of BOOST 24: https://boost24.elicsir.it/home\n","date":"2024-09-18T02:06:52+02:00","image":"https://matteodegobbi.github.io/boost.jpg","permalink":"https://matteodegobbi.github.io/p/summer-school-boost-2024/","title":"Summer school BOOST 2024"},{"content":"Capacitated vehicle routing problem This semester during the artificial intelligence course I developed a programming project to learn more about genetic algorithms.\nI set out to solve the capacitated vehicle routing problem, which is an extenstion of the traveling salesman problem where there are multiple salesmen/trucks that need to deliver some products from a storage facility to clients.\nEach client requests some amount of goods and the trucks have a maximum capacity of goods they can transport. Also the routes of the trucks need to not have any client in common. The objective is to determine the minimum number of trucks necessary to satisfy all the clients and find their routes such that the total number of kilometers travelled is minimized.\nThis is a well known problem in the field of operations research and is very important for the logistics of transportation.\nFinding the optimal solution is hard especially for big numbers of clients, even a number of clients in the hundreds makes finding the optimal solution using ILP solvers infeasible. For this reason heuristic approaches are often used to solve instances of this problem, in particular some options are:\nTabu Search Simulated Annealing Iterated Local Search Genetic Algorithm I decided to use genetic algorithms, I used as genetic operators:\nPartially mapped crossover Alternating edge crossover Elitism, meaning the best chromosome always survives) Random mutation, randomly swapping two genes with a small probability I was able to find good solutions close to the optimal, and in general much better solutions than a greedy approach that always selects the closest client as the next stop for the truck.\nAlso to increase performance of the algorithm I used the library numba which uses a jit compiler to optimize functions when using type hints.\nComparison of a bad and good solution to a VRP instance ","date":"2024-05-18T01:27:57+02:00","image":"https://matteodegobbi.github.io/misc/vrp.jpg","permalink":"https://matteodegobbi.github.io/p/solving-the-vehicle-routing-problem-with-genetic-algorithms/","title":"Solving the Vehicle Routing Problem with genetic algorithms"},{"content":"In my bachelor\u0026rsquo;s thesis I explored the idea of membership inference attacks, where we try to determine whether a given sample was present in the training set of a neural network without having access to the weights of the neural network itself. In particular I targeted Generative Adversarial Networks where the training data can contain sensitive information, especially in the medical setting, in forensics and criminal justice.\nThe full thesis can be found at: https://thesis.unipd.it/handle/20.500.12608/57083\nAbstract: Generative Adversarial Networks (GANs) have had great success in the generation of artifical samples from datasets made of sensitive data which can\u0026rsquo;t be disclosed publicly. These GANs, if released to the public, could allow an attacker to leak sensitive information from the GAN\u0026rsquo;s training dataset. We analize a type of attack called Membership Inference Attack (MIA), which consists of determining the membership of a certain sample to the training set of the GAN. We analize the success of both black box and white box Membership Inference Attacks on GANs trained on MNIST and anime faces. We look for a relationship between the precision of the attacks and the several hyperparameters of the GANs such as: amount of images in the training set, number of epochs of training, quality of generated images, number of generated images available to the attacker. We show how an insufficient number of training images or an excessive number of training epochs causes overfitting in the GAN, which is then vulnerable to MIAs. We analize how the Fr√©chet\u0026rsquo;s Inception Distance (FID) between the set of generated images and the original training set impacts on the success of the MIAs.\n","date":"2023-10-16T02:06:54+02:00","image":"https://matteodegobbi.github.io/gan.png","permalink":"https://matteodegobbi.github.io/p/dataset-inference-attacks-for-generative-adversarial-networks/","title":"Dataset Inference attacks for Generative Adversarial Networks ü§ñ"}]